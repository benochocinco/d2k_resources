---
title: "VAR Example"
author: "D2K Course Staff"
date: "`r format(Sys.time(), '%B %d %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr) # We need the knitr package to set chunk options
library(tidyverse)
library(gridExtra)

# Set default knitr options for knitting code into the report:
opts_chunk$set(echo=FALSE,  # change to FALSE to keep code out of the knitted document
               cache=FALSE, # re-run code that has already been run?
               autodep=TRUE, # assure that caching dependencies are updated correctly
               cache.comments=FALSE, # do not re-run a chunk if only comments are changed
               message=FALSE, # change to FALSE to keep messages out of the knitted document
               warning=FALSE,  # change to FALSE to keep warnings out of the knitted document
               comment = NA,
               tidy.opts=list(width.cutoff=65),
               fig.height = 3.5)

theme1 <- theme_bw() +
  theme(axis.text = element_text(size = 8, colour = "#6b3447"),
        axis.title = element_text(size = 10, colour = "#2f2f63"),
        legend.title = element_text(size = 8, colour = "#2f2f63"),
        legend.text = element_text(size = 8, colour = "#6b3447"), 
        title = element_text(size = 12, colour = "#2f2f63"), 
        axis.ticks = element_line(colour = "#6b3447"),
        plot.caption = element_text(size = 8, colour = "#2f2f63"),
        plot.subtitle = element_text(size = 10, colour = "#2f2f63"))

```


# Introduction

In the traditional multiple linear regression model with $i \in 1, ..., n$ observations, a response variable $\textbf{Y}$ is modeled as a linear combination of predictor variables $\textbf{X}_j$, $j \in 1, ..., p$: $$\textbf{Y}_i = \beta_0 + \beta_1\textbf{X}_{i1} + ... + \beta_p\textbf{X}_{ip} + \epsilon_i,  \epsilon_i \stackrel{iid}{\sim} N(\textbf{0}_p, \sigma^2 \mathbf{I}_p).$$ However, this is subject to the assumption that our observations are independently and identically distributed. One particular case where this assumption is almost surely violated is in the case of time series data. In this setting, we often see that our variables are correlated with themselves at previous time points; this is called *autocorrelation*. Below, we will show an example from the `uschange` data set from the `fpp2` package in R. The data set contains values for 5 quarterly US economic indicators from 1960 to 2016. The model will be fit using the `vars` package.

```{r}
library(fpp2)

data("uschange")
head(uschange)
```


## Autoregressive (AR) Process

For the case of one single variable, we write an autoregressive process for a variable $Y$ as: $$Y_t = \mu + \phi_1 Y_{t-1} + ... + \phi_k Y_{t-k} + \epsilon_t, \epsilon \stackrel{iid}{\sim} N(0, \sigma^2)$$ where the subscript denotes the time point of the observation. Essentially, this means that the value of $Y$ at time $t$ is a function of the value of $Y$ at times $t-1, t-2, ..., t-k$, along with some random Gaussian noise. The difference between the times of the observations is called the *lag*, e.g. $Y_{t-2}$ is considered to be lag 2 for $Y_t$ and to be lag 1 for $Y_{t-1}$. The parameter $k$ denotes the largest lag at which there is a statistically significant autocorrelation with the current time point, and the model is denoted as an AR(k) process. The value of the autocorrelation between $Y_t$ and $Y_{t-i}$ is simply the stanardized version of the parameter $\phi_i$. The key assumptions for this model are stationarity, meaning that the mean and variance of the process does not change, and independence of the residuals $\epsilon_t$.

## Vector Autoregressive (VAR) Process

The vector autoregressive process extends the autoregressive process shown above to multiple variables. For example, if we have 3 variables $X$, $Y$, and $Z$, we write the model as: $$X_t = \mu_X + \gamma_{X,1} Y_{t-1} + ... + \gamma_{X,k} Y_{t-k} + \alpha_{X,1} X_{t-1} + ... + \alpha_{X,1} X_{t-k} + \beta_{X,1} Z_{t-1} +... + \beta_{X,k} Z_{t-k} + \epsilon_{X,t}, \epsilon_X \stackrel{iid}{\sim} N(0, \sigma^2_X)$$ $$Y_t = \mu_Y + \gamma_{Y,1}  Y_{t-1} + ... + \gamma_{Y,k}  Y_{t-k} + \alpha_{Y,1} X_{t-1} + ... + \alpha_{Y,k}  X_{t-k} + \beta_{Y,1} Z_{t-1} +... + \beta_{Y,k} Z_{t-k} + \epsilon_{Y,t}, \epsilon_Y \stackrel{iid}{\sim} N(0, \sigma^2_Y)$$ $$Z_t = \mu_Z + \gamma_{Z,1} Y_{t-1} + ... + \gamma_{Z,k} Y_{t-k} + \alpha_{Z,1} X_{t-1} + ... + \alpha_{Z,1} X_{t-k} + \beta_{Z,1} Z_{t-1} +... + \beta_{Z,k} Z_{t-k} + \epsilon_{Z,t}, \epsilon_Z \stackrel{iid}{\sim} N(0, \sigma^2_Z).$$ As above, the parameter $k$ denotes the maximum time lags with significant parameters for $X$, $Y$. and $Z$ (we have the variables have the same maximum lag). This is called a VAR(k) model. The assumption for stationarity is required here as well for all variables. The residuals $\epsilon_X, \epsilon_Y$ and $\epsilon_Z$ need to be independently and identically distributed with respect to time, but they do not necessarily need to independent of one another contemporaneously. In other words, the model is valid if $\epsilon_{X,t}$ is correlated with $\epsilon_{Y,t}$ but not if $\epsilon_{X,t}$ is correlated with $\epsilon_{X,t-1}$. In fact, we can consider contemporaneous correlations between the residuals for each of the variables as a measure of the relative strength of the relationship between the variables at the same current time point.

\newpage 

# Example

## Model Fit

We fit a VAR(1) on the 5 variabls in the `uschange` data. The summary of the fit is shown below. In total, we will have 30 different estimated coefficients - the 5 variables at lag 1 and an intercept predicting each of the variables in the data set. At the bottom, we also see the covariance and correlation matrix of the residuals.

```{r}
library(vars)
mod1 <- VAR(uschange)

summary(mod1)
```

## Diagnostics

As when we fit any model with assumptions, we need to check the validity of our estimates.

### Stationarity

If the data are truly stationary, then the time series should not "drift" away from the global mean and the variance of the process should not change over time. The graphs below seem to indicate, at least visually, that the stationarity assumption is not violated. Mean stationarity can be tested formally with the Augmented Dickey-Fuller (ADF) test (not shown in the example here); a low p-value indicates stationarity.

```{r}
ggplot() +
  geom_path(aes(x = seq(1970, 2016.5, 0.25), y = uschange[, 1])) +
  geom_hline(yintercept = mean(uschange[, 1]), col = "red") +
  labs(title = "Consumption",
       x = "Time",
       y = "% Change, Quarterly Consumption Expenditure (USD)") +
  theme1
```

```{r}
ggplot() +
  geom_path(aes(x = seq(1970, 2016.5, 0.25), y = uschange[, 2])) +
  geom_hline(yintercept = mean(uschange[, 2]), col = "red") +
  labs(title = "Income",
       x = "Time",
       y = "% Change, Quarterly Personal Disposable Income (USD)") +
  theme1
```

```{r}
ggplot() +
  geom_path(aes(x = seq(1970, 2016.5, 0.25), y = uschange[, 3])) +
  geom_hline(yintercept = mean(uschange[, 3]), col = "red") +
  labs(title = "Production",
       x = "Time",
       y = "% Change, Quarterly Production Rate") +
  theme1
```

```{r}
ggplot() +
  geom_path(aes(x = seq(1970, 2016.5, 0.25), y = uschange[, 4])) +
  geom_hline(yintercept = mean(uschange[, 4]), col = "red") +
  labs(title = "Savings",
       x = "Time",
       y = "% Change, Quarterly Savings Rate") +
  theme1
```

```{r}
ggplot() +
  geom_path(aes(x = seq(1970, 2016.5, 0.25), y = uschange[, 5])) +
  geom_hline(yintercept = mean(uschange[, 5]), col = "red") +
  labs(title = "Unemployment",
       x = "Time",
       y = "% Change, Quarterly Unemployment Rate") +
  theme1
```

### Autocorrelation

We also need to check that the residuals do not show any autocorrelation structure. To do this, we plot what are called the *autocorrelation function* and *partial autocorrelation function*. Autocorrelation is defined above in the introduction; partial autocorrelation is simply autocorrelation conditional on all previous time lags. The graphs simply show the autocorreation and partial autocorrelations for the residuals at different lags. If there are any large positive or negative values (besides at lag = 0) in the graph, there could still be some unexplained time dependent structure that has not been accounted for in the model as currently constructed.

```{r}
acf(resid(mod1$varresult$Income),
    main = "ACF of Income Residuals")

pacf(resid(mod1$varresult$Income),
    main = "PACF of Income Residuals")
```

## Inference

Since the residuals are assumed to be distributed from a multivariate Gaussian distribution, we can use the typical linear regression hypothesis tests and p-values to evaluate the significance of the estimated parameters. As such, we can reliably use the p-values in the output above for testing individual parameters, and we an also use F-tests to test a whole set of parameters, e.g. one whole time lag or one entire variable in the model. In the model above, we can see that, for example, the percent change in personal disposable income in the current quarter is significantly related to the percent change in production rate and personal consumption expenditure in the previous quarter. On the other hand, the percent change in quarterly personal consumption expenditure for this quarter only seems to related to the change in quarterly personal consumption expenditure for the previous quarter.

## Forecasting

Predictions, along with confidence intervals, can be extracted as well: 

```{r}
predict(mod1, n.ahead = 4)
```

This shows the forecast for the economic indicators for the next 4 quarters after the data set ends. Notice that the width of the confidence intervals gets wider the further out we go - this is typical, since any error in predition will propagate through time.

\newpage

# Assumption Violation

So what do we do if the assumptions of the VAR model are violated in one or more of the raw variables? In the case of heteroskedacity, we can attempt to apply the same tricks that we do in linear regression - some kind of transformation on the individual variables so that the variance remains constant throughout. In the case of a changing mean, we have several options:

- If the mean is changing linearly, we can use linear regression with respect to time and subtract the predicted mean out of the data.
- If the mean is changing nonlinearly, we can use some kind of nonparameteric regression method, such as a spline or a loess regression,  with respect to time to predict the changing mean. Again, we then subtract the mean out of the data.
- We can estimate the changing mean using some kind of smoothing algorithm, such as kernel smoothing, exponential smoothing, or Holt's method, then subtract that mean out of the data.
- We can apply differencing, which simply means taking the difference between consecutive observations. This will create a new variables $\Delta Y_{t-1} = Y_t - Y_{t-1}, t \in 2, ..., n$ that we can then use.

